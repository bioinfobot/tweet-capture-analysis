{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library.\n",
    "import re\n",
    "from twitterfunc import tweet_clean\n",
    "import datetime\n",
    "import sqlite3\n",
    "from collections import OrderedDict\n",
    "\n",
    "# External library.\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from wordcloud import WordCloud\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_value_sort_return_top(frquency_dict, maxreturn):\n",
    "    \"\"\"Sort the dictionary according to values and return a list of top n elements\"\"\"\n",
    "    dictionary_sorted = OrderedDict(sorted(frquency_dict.items(), key=lambda t: t[1], reverse=True))\n",
    "    # Store top values in an array\n",
    "    # Change maxCount value to extract top n elements\n",
    "    count = 0\n",
    "    top_elements = []\n",
    "    for k, v in dictionary_sorted.items():\n",
    "        # Key and value pairs are stored in the form of a tuple in the topWords array\n",
    "        # Another dictionary is not created here in order to preserve the sorted order\n",
    "        top_elements.append((k, v))\n",
    "        count += 1\n",
    "        if count >= maxreturn:\n",
    "            break\n",
    "    return top_elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A.NET (A#/A sharp)', 0), ('A-0 System', 0), ('A+ (A plus)', 0), ('ABAP', 0), ('ABC', 0), ('ABC ALGOL', 0), ('ACC', 0), ('Accent (Rational Synergy)', 0), ('Ace DASL (Distributed Application Specification Language)', 0), ('Action!', 0), ('ActionScript', 0), ('Actor', 0), ('Ada', 0), ('Adenine (Haystack)', 0), ('AdvPL', 0), ('Agda', 0), ('Agilent VEE (Keysight VEE)', 0), ('Agora', 0), ('AIMMS', 0), ('Aldor', 0)]\n"
     ]
    }
   ],
   "source": [
    "# Establish connection to Sqlite3 database.\n",
    "conn = sqlite3.connect('./bioinfotweet.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "# Extract tweets' text from the database followed by filtering and tokenizing.\n",
    "filteredText = []\n",
    "rowCount = 0\n",
    "totalHash = []\n",
    "totalUsers = []\n",
    "totalTweetID = []\n",
    "prog_lang = []\n",
    "total_lang = {}\n",
    "\n",
    "# Calculate date.\n",
    "current_utc = str(datetime.datetime.now(datetime.timezone.utc))\n",
    "year = current_utc[0:4]\n",
    "month = current_utc[5:7]\n",
    "\n",
    "if month == 1: # This is to check for the year change (January month)\n",
    "    month = 12\n",
    "    year = int(year) - 1\n",
    "else:\n",
    "    month = int(month) - 1\n",
    "\n",
    "name = \"{0}-{1}\".format(year, str(month).zfill(2)) # name here means yyyy-mm\n",
    "\n",
    "# NOTE: update here\n",
    "# Open txt file with the list of programming languages\n",
    "file_prog = json.load(json_obj)\n",
    "for lang, link in file_prog.items():\n",
    "    prog_lang.append(lang.rstrip())\n",
    "    tempdict = {lang.rstrip(): 0}\n",
    "    total_lang.update(tempdict)\n",
    "    \n",
    "for ha in totalHash:\n",
    "    for pro in prog_lang:\n",
    "        if \"#\"+pro.lower() == ha:\n",
    "            total_lang[pro] += 1\n",
    "\n",
    "\n",
    "totalWords = len(filteredText)\n",
    "freq = FreqDist(filteredText)\n",
    "uniqueWords = len(freq)\n",
    "del filteredText\n",
    "\n",
    "stopHash = ['#twitter', '#tweeted']  # Hastags of no interest\n",
    "totalHash[:] = [h for h in totalHash if h not in stopHash]  # Get ride of any cell with stop hashtags\n",
    "hashFreq = FreqDist(totalHash)\n",
    "usersFreq = FreqDist(totalUsers)\n",
    "lang_freq = FreqDist(total_lang)\n",
    "\n",
    "# # Generate a word cloud image.\n",
    "# wordcloud = WordCloud(font_path='Actor-Regular.ttf', width=1500, height=500,\n",
    "#                       max_words=500, stopwords=None, background_color='whitesmoke',\n",
    "#                       max_font_size=None, font_step=1, mode='RGB',\n",
    "#                       collocations=True, colormap=None, normalize_plurals=True).generate_from_frequencies(freq)\n",
    "# imagePath = \"/home/bioinformaticsbot/bioinfobot.github.io/images/\" + name + '.png'  # Put the actual path of the word cloud image produced in the previous step\n",
    "# wordcloud.to_file(imagePath)\n",
    "# imageUrl = \"https://bioinfobot.github.io/images/\" + name + '.png'\n",
    "\n",
    "\n",
    "\n",
    "# # Sort and store top n elements in an array\n",
    "topWords = dict_value_sort_return_top(freq, 20)\n",
    "del freq  # Delete freq variable to free memory space\n",
    "hashFreqSorted = dict_value_sort_return_top(hashFreq, 20)\n",
    "del hashFreq\n",
    "usersFreqSorted = dict_value_sort_return_top(usersFreq, 20)\n",
    "del usersFreq\n",
    "lang_freq_sorted = dict_value_sort_return_top(lang_freq, 20)\n",
    "del lang_freq\n",
    "lang_nonzero = []\n",
    "for l in range(len(lang_freq_sorted)):\n",
    "    if lang_freq_sorted[l][1] > 0:\n",
    "        lang_nonzero.append(lang_freq_sorted[l])\n",
    "        print(total_lang)\n",
    "print(lang_freq_sorted)\n",
    "# mainJsonDump = {'PopularLanguages': lang_nonzero}\n",
    "# # Write a json file\n",
    "# # jsonPath = '/home/bioinformaticsbot/bioinfobot.github.io/data/' + name + '.json'\n",
    "# jsonPath = \"/\" +name + '.json'\n",
    "# with open(jsonPath, 'w') as wcd:\n",
    "#     json.dump(mainJsonDump, wcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'python'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10700/4266123740.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtotal_lang\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"python\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'python'"
     ]
    }
   ],
   "source": [
    "total_lang[\"python\"]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cf5a7bc93abf8bde270df3a8cb4317d25d75eb3d12387d6efd9dcca903b78a85"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('bioinfobot': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
