{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_webpage():\n",
    "    link = \"https://en.wikipedia.org/wiki/List_of_programming_languages\"\n",
    "    r = requests.get(link)\n",
    "    if r.status_code != 200:\n",
    "        raise ConnectionError(\"Non 200 code was capture {}\".format(r.status_code))\n",
    "\n",
    "    content = r.text\n",
    "    return content\n",
    "\n",
    "def parse_webpage(): \n",
    "    content = get_webpage()\n",
    "    bs = BeautifulSoup(content)\n",
    "\n",
    "    #fdsfds\n",
    "    wiki_link = \"https://en.wikipedia.org\"\n",
    "    \n",
    "    # selecting container that has all programing languages\n",
    "    lang_elm_id = \"mw-content-text\"\n",
    "    lang_table = bs.find(id=lang_elm_id)\n",
    "\n",
    "    # selecing all the columns\n",
    "    langs = defaultdict(lambda: None)\n",
    "    divs = lang_table.find_all(\"div\", {\"class\":\"div-col\"})\n",
    "    for div in divs:\n",
    "        list_elm = div.find_all(\"ul\")\n",
    "        for ul in list_elm:\n",
    "            list_items = ul.find_all(\"li\")\n",
    "            for li in list_items:\n",
    "                # error handlin when a link is not found \n",
    "                # -- string manipulation to remove unicode characters\n",
    "                try:\n",
    "                    endpoint = li.find(\"a\").get(\"href\")\n",
    "                    link = \"{}{}\".format(wiki_link, endpoint)\n",
    "                    lang = li.text.encode(\"ascii\", \"ignore\").decode(\"utf-8\")\n",
    "                    langs[lang] = link \n",
    "                except AttributeError:\n",
    "                    # some languages do not links so only append \n",
    "                    link = \"N/A\"\n",
    "                    lang = li.text.encode(\"ascii\", \"ignore\").decode(\"utf-8\")\n",
    "                    langs[lang] = link \n",
    "                \n",
    "    return langs\n",
    "data = parse_webpage()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"langs.json\", \"w\") as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cf5a7bc93abf8bde270df3a8cb4317d25d75eb3d12387d6efd9dcca903b78a85"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('bioinfobot': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
